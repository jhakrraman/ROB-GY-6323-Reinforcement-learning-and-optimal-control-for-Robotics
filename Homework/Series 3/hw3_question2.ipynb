{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Iteration took 1376 iterations to converge\n",
      "\n",
      "Final Value Function:\n",
      "[[-95.07940237 -96.03980137 -97.00990137 -98.99990137 -99.99990137]\n",
      " [-94.12860736   0.         -96.02980137 -98.00990137 -88.99990137]\n",
      " [-93.1873203  -82.25544611   0.         -97.00990137 -98.99990137]\n",
      " [-91.25544611 -90.34289066 -89.43946077 -88.99990137 -99.99990137]]\n"
     ]
    }
   ],
   "source": [
    "# Definition of grid world parameters\n",
    "rows, cols = 4, 5\n",
    "obstacles = [(1, 1), (2, 2)]\n",
    "value_function = np.zeros((rows, cols))\n",
    "\n",
    "# Grid costs\n",
    "grid = np.zeros((rows, cols))\n",
    "# Violet cells (-1)\n",
    "grid[0, 4] = -1\n",
    "grid[3, 4] = -1\n",
    "# Green cells (1)\n",
    "grid[0, 2] = 1\n",
    "grid[1, 2] = 1\n",
    "grid[2, 3] = 1\n",
    "grid[3, 0] = 1\n",
    "# Red cells (10)\n",
    "grid[2, 1] = 10\n",
    "grid[1, 4] = 10\n",
    "grid[3, 3] = 10\n",
    "\n",
    "alpha = 0.99\n",
    "threshold = 1e-6\n",
    "actions = [(0,0), (0,1), (0,-1), (1,0), (-1,0)]\n",
    "\n",
    "def run_value_iteration():\n",
    "    value_function = np.zeros((rows, cols))\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        iterations += 1\n",
    "        delta = 0\n",
    "        new_value = np.copy(value_function)\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if (i,j) in obstacles:\n",
    "                    continue\n",
    "                    \n",
    "                values = []\n",
    "                for action in actions:\n",
    "                    next_i = i + action[0]\n",
    "                    next_j = j + action[1]\n",
    "                    \n",
    "                    if (0 <= next_i < rows and \n",
    "                        0 <= next_j < cols and \n",
    "                        (next_i, next_j) not in obstacles):\n",
    "                        values.append(grid[i,j] + alpha * value_function[next_i, next_j])\n",
    "                \n",
    "                if values:\n",
    "                    new_value[i,j] = min(values)\n",
    "                    delta = max(delta, abs(new_value[i,j] - value_function[i,j]))\n",
    "        \n",
    "        value_function = new_value\n",
    "        if delta < threshold:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\nValue Iteration took {iterations} iterations to converge\")\n",
    "    print(\"\\nFinal Value Function:\")\n",
    "    print(value_function)\n",
    "    return iterations, value_function\n",
    "\n",
    "# Run Value Iteration\n",
    "vi_iterations, vi_value = run_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Iteration Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Iteration took 8 iterations to converge\n",
      "\n",
      "Final Value Function:\n",
      "[[-95.07949251 -96.03989151 -97.00999151 -98.99999151 -99.99999151]\n",
      " [-94.12869758   0.         -96.02989159 -98.00999159 -88.99999159]\n",
      " [-93.18741061 -82.2555365    0.         -97.00999151 -98.99999151]\n",
      " [-91.2555365  -90.34298114 -89.43955132 -88.99999151 -99.99999151]]\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration Implementation\n",
    "def run_policy_iteration():\n",
    "    # Initialization of the policy to \"do nothing\" (0,0)\n",
    "    policy = np.zeros((rows, cols, 2), dtype=int)\n",
    "    value = np.zeros((rows, cols))\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        iterations += 1\n",
    "        # Policy evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    if (i,j) in obstacles:\n",
    "                        continue\n",
    "                        \n",
    "                    old_value = value[i,j]\n",
    "                    next_i = i + policy[i,j,0]\n",
    "                    next_j = j + policy[i,j,1]\n",
    "                    \n",
    "                    if (0 <= next_i < rows and \n",
    "                        0 <= next_j < cols and \n",
    "                        (next_i, next_j) not in obstacles):\n",
    "                        value[i,j] = grid[i,j] + alpha * value[next_i, next_j]\n",
    "                    \n",
    "                    delta = max(delta, abs(value[i,j] - old_value))\n",
    "            \n",
    "            if delta < threshold:\n",
    "                break\n",
    "                \n",
    "        # Policy improvement\n",
    "        policy_stable = True\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if (i,j) in obstacles:\n",
    "                    continue\n",
    "                    \n",
    "                old_action = policy[i,j].copy()\n",
    "                values = []\n",
    "                \n",
    "                for action in actions:\n",
    "                    next_i = i + action[0]\n",
    "                    next_j = j + action[1]\n",
    "                    \n",
    "                    if (0 <= next_i < rows and \n",
    "                        0 <= next_j < cols and \n",
    "                        (next_i, next_j) not in obstacles):\n",
    "                        values.append((grid[i,j] + alpha * value[next_i, next_j], action))\n",
    "                \n",
    "                if values:\n",
    "                    best_value, best_action = min(values)\n",
    "                    policy[i,j] = best_action\n",
    "                    \n",
    "                if not np.array_equal(policy[i,j], old_action):\n",
    "                    policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nPolicy Iteration took {iterations} iterations to converge\")\n",
    "    print(\"\\nFinal Value Function:\")\n",
    "    print(value)\n",
    "    return iterations, value, policy\n",
    "\n",
    "# Run Policy Iteration\n",
    "pi_iterations, pi_value, pi_policy = run_policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
